{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "401abc6e974ba9e33a7fb989e4f418788c2a6e11"
   },
   "source": [
    "# 如何使用推荐工具以及推荐算法进行推荐\n",
    "\n",
    "**常用的推荐算法包括基于SVD家族的协同过滤算法，基于FM，DeepFM等的二阶交互模型**\n",
    "\n",
    "这个notebook尽可能的实现这些基本方法并利用RMSE对算法进行评估。\n",
    "\n",
    "此notebook是依据如下的notebooks进行了修改:\n",
    "**[morrisb](https://www.kaggle.com/morrisb/how-to-recommend-anything-deep-recommender)**,**[siavrez](https://www.kaggle.com/siavrez/deepfm-model)**\n",
    "***\n",
    "+ [1. 加载库文件](#1)<br>\n",
    "+ [2. 加载Item文件](#2)<br>\n",
    "+ [3. 加载User文件](#3)<br>\n",
    "+ [4. 过滤稀疏的User和Item](#4)<br>\n",
    "+ [5. 创建训练和测试集](#5)<br>\n",
    "+ [6. 转换User-Ratings到User-Item-Rating-Matrix](#6)<br>\n",
    "+ [7. 推荐算法引擎](#7)<br>\n",
    " + [7.1. Mean Rating](#7.1)<br>\n",
    " + [7.2. Weighted Mean Rating](#7.2)<br>\n",
    " + [7.3. Cosine User-User Similarity](#7.3)<br>\n",
    " + [7.4. Matrix Factorisation With Keras And Gradient Descent](#7.5)<br>\n",
    " + [7.5. Deep Learning With Keras](#7.6)<br>\n",
    "+ [8. Exploring Python Libraries](#8)<br>\n",
    " + [8.1. Surprise Library](#8.1)<br>\n",
    " + [8.2. Lightfm Library](#8.2)<br>\n",
    " + [8.3. Deepctr Library](#8.3)<br>\n",
    "+ [9. Conclusion](#9)<br>\n",
    "***\n",
    "## <a id=1>1. 加载库文件</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepctr\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/8e/03d45ded03d594212003801e2b4af0927b66575741fd6df72a07fb6affd3/deepctr-0.7.2-py3-none-any.whl (79kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 3.1MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from deepctr) (2.8.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from deepctr) (2.18.4)\r\n",
      "Requirement already satisfied: numpy>=1.7 in /opt/conda/lib/python3.6/site-packages (from h5py->deepctr) (1.15.2)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from h5py->deepctr) (1.11.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->deepctr) (2018.8.24)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->deepctr) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->deepctr) (2.6)\r\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->deepctr) (1.22)\r\n",
      "\u001b[31mmxnet 1.3.0.post0 has requirement numpy<1.15.0,>=1.8.2, but you'll have numpy 1.15.2 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mkmeans-smote 0.1.0 has requirement imbalanced-learn<0.4,>=0.3.1, but you'll have imbalanced-learn 0.5.0.dev0 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mkmeans-smote 0.1.0 has requirement numpy<1.15,>=1.13, but you'll have numpy 1.15.2 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31manaconda-client 1.7.2 has requirement python-dateutil>=2.6.1, but you'll have python-dateutil 2.6.0 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mimbalanced-learn 0.5.0.dev0 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: deepctr\r\n",
      "Successfully installed deepctr-0.7.2\r\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install deepctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# To store the data\n",
    "import pandas as pd\n",
    "\n",
    "# To do linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# To create plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # To create interactive plots\n",
    "# from plotly.offline import init_notebook_mode, plot, iplot, download_plotlyjs\n",
    "# import plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "# # init_notebook_mode(connected=True)\n",
    "# To operator files\n",
    "import os\n",
    "# To shift lists\n",
    "from collections import deque\n",
    "\n",
    "# To compute similarities between vectors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# To use recommender systems\n",
    "import surprise as sp\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# To create deep learning models\n",
    "from keras.layers import Input, Embedding, Reshape, Dot, Concatenate, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# To create sparse matrices\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# To light fm\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "\n",
    "# To deepctr\n",
    "from deepctr.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr.models import DeepFM, xDeepFM, DCN, DIN, DSIN, DIEN\n",
    "\n",
    "# To stack sparse matrices\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "81ecc38ecc2650b81c042a385599a3af31b4e1e6"
   },
   "source": [
    "## <a id=2>2. 加载Item文件</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qualifying.txt',\n",
       " 'movie_titles.csv',\n",
       " 'combined_data_4.txt',\n",
       " 'combined_data_2.txt',\n",
       " 'README',\n",
       " 'combined_data_1.txt',\n",
       " 'combined_data_3.txt',\n",
       " 'probe.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载netflix-prize-data数据集\n",
    "os.listdir('../input/netflix-prize-data/')\n",
    "# qualifying.txt:要提交的预测文件\n",
    "# MovieID1:\n",
    "# CustomerID11,Date11\n",
    "# CustomerID12,Date12\n",
    "# -> \n",
    "# MovieID1:\n",
    "# Rating11\n",
    "# Rating12\n",
    "\n",
    "# probe.txt: 和qualifying.txt文件类似，与之不同的是没有Date列\n",
    "\n",
    "# movie_titles.txt : 电影信息，数据格式为MovieId, YearOfRelease, Title\n",
    "# combined_data_1/2/3/4.txt ： 训练集， 数据格式为CustomerID(user), Rating, Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a698fdfcf9ac8ef2193c3b40503b92283c5bec8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Movie-Titles:\t(17770, 2) \n",
      " Contains 17770 items\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16623</th>\n",
       "      <td>1976.0</td>\n",
       "      <td>Bugsy Malone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5950</th>\n",
       "      <td>2004.0</td>\n",
       "      <td>Evel Knievel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12808</th>\n",
       "      <td>1997.0</td>\n",
       "      <td>Velocity Trap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>2004.0</td>\n",
       "      <td>The Life Aquatic with Steve Zissou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>1996.0</td>\n",
       "      <td>Crash Dive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Year                                Name\n",
       "Id                                               \n",
       "16623  1976.0                        Bugsy Malone\n",
       "5950   2004.0                        Evel Knievel\n",
       "12808  1997.0                       Velocity Trap\n",
       "1719   2004.0  The Life Aquatic with Steve Zissou\n",
       "379    1996.0                          Crash Dive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_netflix = pd.read_csv('../input/netflix-prize-data/movie_titles.csv', \n",
    "                           encoding = 'ISO-8859-1', \n",
    "                           header = None, \n",
    "                           names = ['Id', 'Year', 'Name']).set_index('Id')\n",
    "\n",
    "print('Shape Movie-Titles:\\t{} \\n Contains {} items'.format(movie_netflix.shape, movie_netflix.shape[0]))\n",
    "movie_netflix.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载the-movies-dataset数据集\n",
    "# os.listdir('../input/the-movies-dataset')\n",
    "# movies_metadata.csv: 电影元文件，每个电影共计24个特征\n",
    "# keywords.csv: id-keyword，每个电影对应一个关键词\n",
    "# credits.csv: id-cast-crew，每个电影对应摄制组和演员信息\n",
    "# links.csv: id-imdbid-tmdbid，不同电影平台对同一部电影的不用标识\n",
    "# ratings_small.csv : 评分数据，userId-movieId-rating-timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "42c7f5621f216b7300b5143a8e68d9e6d495e89f"
   },
   "outputs": [],
   "source": [
    "# low_memory=False关键词\n",
    "# low_memory=False 参数设置后，pandas会一次性读取csv中的所有数据，然后对字段的数据类型进行唯一的一次猜测。这样就不会导致同一字段的Mixed types问题了。\n",
    "# 但是这种方式真的非常不好，一旦csv文件过大，就会内存溢出；\n",
    "# movie_metadata = pd.read_csv('../input/the-movies-dataset/movies_metadata.csv', low_memory=False)[['original_title', 'id', 'release_date', 'vote_count']].set_index('id')\n",
    "# # 移除投票次数小于10的样本\n",
    "# movie_metadata = movie_metadata[movie_metadata['vote_count']>10].drop('vote_count', axis=1)\n",
    "\n",
    "# print('Shape Movie-Metadata:\\t{}\\n Contains {} items'.format(movie_metadata.shape, movie_metadata.shape[0]))\n",
    "# movie_metadata.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载movielens20m数据集\n",
    "# os.listdir('../input/movielens-20m-dataset/')\n",
    "# tag.csv: userId-movieId-tag-timestamp\n",
    "# rating.csv: userId-movieId-rating-timestamp\n",
    "# movie.csv: movieId-title-genres\n",
    "# link.csv: moiveId-imdbId-tmbdId\n",
    "# genome_scores.csv: movieId-tagId-relevance\n",
    "# genome_tags.csv: tagId-tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_movielens = pd.read_csv('../input/movielens-20m-dataset/movie.csv').set_index('movieId')\n",
    "# print('Shape MovieLens-movice:\\t{}\\n Contains {} items'.format(movie_movielens.shape, movie_movielens.shape[0]))\n",
    "# movie_movielens.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=3>3. 加载User文件</a>\n",
    "其中每条user样本，都类似关联算法中的transaction。统一User-Item-Rating的columns为userId-itemId-rating。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "cf6473e25f7fd85d4896e1a87fd92b51f26fafa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Raw Data:\t(24058263, 3)\n",
      "Shape User-Ratings:\t(24053764, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>Date</th>\n",
       "      <th>movieId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9926947</th>\n",
       "      <td>211138</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2005-07-26</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14404112</th>\n",
       "      <td>377942</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-07-07</td>\n",
       "      <td>2782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14557200</th>\n",
       "      <td>765929</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2003-12-26</td>\n",
       "      <td>2800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21580856</th>\n",
       "      <td>2460896</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-04-06</td>\n",
       "      <td>4056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6409512</th>\n",
       "      <td>1996259</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2003-10-02</td>\n",
       "      <td>1255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userId  rating        Date  movieId\n",
       "9926947    211138     3.0  2005-07-26     1925\n",
       "14404112   377942     4.0  2005-07-07     2782\n",
       "14557200   765929     4.0  2003-12-26     2800\n",
       "21580856  2460896     4.0  2005-04-06     4056\n",
       "6409512   1996259     4.0  2003-10-02     1255"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load single data-file \n",
    "# combined_data_1 = pd.read_csv('../input/netflix-prize-data/combined_data_1.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n",
    "# combined_data_2 = pd.read_csv('../input/netflix-prize-data/combined_data_2.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n",
    "# combined_data_3 = pd.read_csv('../input/netflix-prize-data/combined_data_3.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n",
    "# combined_data_4 = pd.read_csv('../input/netflix-prize-data/combined_data_4.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n",
    "# df_raw = pd.cocat([combined_data_1, combined_data_2, combined_data_3, combined_data_4], axis=0).reset_index()\n",
    "# 鉴于netflix-prize-data中存在\n",
    "df_raw = pd.read_csv('../input/netflix-prize-data/combined_data_1.txt', header=None, names=['userId', 'rating', 'Date'], usecols=[0, 1, 2])\n",
    "print('Shape Raw Data:\\t{}'.format(df_raw.shape))\n",
    "\n",
    "# Find empty rows to slice dataframe for each movie\n",
    "# 编码思路是先找出缺失值的索引，然后遍历过滤掉索引值\n",
    "tmp_movies = df_raw[df_raw['rating'].isna()]['userId'].reset_index()\n",
    "movie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values] # drop ':'\n",
    "\n",
    "# Shift the movie_indices by one to get start and endpoints of all movies\n",
    "shifted_movie_indices = deque(movie_indices)\n",
    "shifted_movie_indices.rotate(-1)  # the first element turn to the last element.\n",
    "\n",
    "\n",
    "# Gather all dataframes\n",
    "user_data = []\n",
    "\n",
    "# Iterate over all movies\n",
    "for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n",
    "    \n",
    "    # Check if it is the last movie in the file\n",
    "    if df_id_1<df_id_2:\n",
    "        tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n",
    "    else:\n",
    "        tmp_df = df_raw.loc[df_id_1+1:].copy()\n",
    "        \n",
    "    # Create movie_id column\n",
    "    tmp_df['movieId'] = movie_id\n",
    "    \n",
    "    # Append dataframe to list\n",
    "    user_data.append(tmp_df)\n",
    "\n",
    "# Combine all dataframes\n",
    "netflix_prize_User = pd.concat(user_data)\n",
    "del user_data, df_raw, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\n",
    "print('Shape User-Ratings:\\t{}'.format(netflix_prize_User.shape))\n",
    "netflix_prize_User.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_dataset_User = pd.read_csv('../input/the-movies-dataset/ratings.csv', low_memory=False)\n",
    "# print('Shape User-Ratings:\\t{}'.format(movie_dataset_User.shape))\n",
    "# movie_dataset_User.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movielens_movie_User = pd.read_csv('../input/movielens-20m-dataset/rating.csv')\n",
    "# print('Shape MovieLens-movice:\\t{}'.format(movielens_movie_User.shape))\n",
    "# movielens_movie_User.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=4>4. 过滤稀疏的User和Item</a>\n",
    "对于user，过滤其与评分系统交互较少的user，即评分的item数量较少；对于item，其被user评过分的次数较少。（其主要目的是为了方便测试，在实验生产环境中，应该对稀疏的user和item做特殊处理，如使用LR模型，深度模型等）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_user_item(user_item_rating, min_nb_item_ratings=300, min_nb_user_ratings=200):\n",
    "    filter_items = (user_item_rating['movieId'].value_counts() > min_nb_item_ratings)\n",
    "    filter_items = filter_items[filter_items].index.tolist()\n",
    "    \n",
    "    filter_users = (user_item_rating['userId'].value_counts() > min_nb_user_ratings)\n",
    "    filter_users = filter_users[filter_users].index.tolist()\n",
    "    filter_ret = user_item_rating[(user_item_rating['movieId'].isin(filter_items)) & (user_item_rating['userId'].isin(filter_users))]\n",
    "    print('Shape User-Ratings unfiltered:\\t{}'.format(user_item_rating.shape))\n",
    "    print('Shape User-Ratings filtered:\\t{}'.format(filter_ret.shape))\n",
    "    return filter_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "6823a75ca1ef06ad4ecc24538f50094493cf5444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape User-Ratings unfiltered:\t(24053764, 4)\n",
      "Shape User-Ratings filtered:\t(6168476, 4)\n"
     ]
    }
   ],
   "source": [
    "# netflix_prize_User\n",
    "filtered_netflix_prize_User = filter_user_item(netflix_prize_User)\n",
    "# filtered_movie_dataset_User = filter_user_item(movie_dataset_User)\n",
    "# filtered_movielens_movie_User = filter_user_item(movielens_movie_User)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del netflix_prize_User#, movie_dataset_User, movielens_movie_User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e41a6237e811e7ff9f1052f49ddac0e8a782145"
   },
   "source": [
    "## <a id=5>5. 创建训练和测试集</a>\n",
    "创建训练集和测试集的目的在于使用推荐系统测评指标进行验证模型的性能，鉴于rating是一个连续值，可以采用RMSE度量方式，即\n",
    "$$RMSE(root\\ square\\ error)=\\sqrt{\\frac{\\sum (y_i-z_i)^2}{N}}$$\n",
    "其中$y_i$表示真实值，$z_i$表示验证值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(filtered_user_item, test_size=0.5):\n",
    "    X_train, X_test, _, _ = train_test_split(filtered_user_item.reset_index(), filtered_user_item['movieId'].values, test_size=test_size, random_state=2020, stratify=filtered_user_item['movieId'].values)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "5d286e3c457b425123827ccd8ab50eb62cf83530"
   },
   "outputs": [],
   "source": [
    "# train_data1, test_data1 = get_train_test(filtered_movie_dataset_User)\n",
    "# movieId1 = train_data1.movieId\n",
    "# userId1 = train_data1.userId\n",
    "# train_data2, test_data2 = get_train_test(filtered_movielens_movie_User)\n",
    "# movieId2 = train_data2.movieId\n",
    "# userId2 = train_data2.userId\n",
    "train_data3, test_data3 = get_train_test(filtered_netflix_prize_User)\n",
    "movieId3 = train_data3.movieId\n",
    "userId3 = train_data3.userId\n",
    "# del filtered_movie_dataset_User, filtered_movielens_movie_User, filtered_netflix_prize_User\n",
    "# del filtered_netflix_prize_User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b574d9a5f28c6ef9e3fb5f016d3a32aa0d0f8c8a"
   },
   "source": [
    "## <a id=6>6. 转换User-Ratings到User-Item-Rating-Matrix</a>\n",
    "转换矩阵使得DataFrame是以userId为index，itemId为columns，其中矩阵中每个值对应rating（即评分）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a3a71c8f19e9f09ad07965cf2ee59a47b472b278"
   },
   "outputs": [],
   "source": [
    "def get_user_item_rating_mat(data):\n",
    "    return data.pivot_table(index='userId', columns='movieId', values='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>8</th>\n",
       "      <th>12</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>50</th>\n",
       "      <th>52</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>68</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>...</th>\n",
       "      <th>4440</th>\n",
       "      <th>4441</th>\n",
       "      <th>4442</th>\n",
       "      <th>4444</th>\n",
       "      <th>4447</th>\n",
       "      <th>4448</th>\n",
       "      <th>4449</th>\n",
       "      <th>4450</th>\n",
       "      <th>4451</th>\n",
       "      <th>4452</th>\n",
       "      <th>4454</th>\n",
       "      <th>4456</th>\n",
       "      <th>4459</th>\n",
       "      <th>4460</th>\n",
       "      <th>4461</th>\n",
       "      <th>4463</th>\n",
       "      <th>4465</th>\n",
       "      <th>4467</th>\n",
       "      <th>4468</th>\n",
       "      <th>4470</th>\n",
       "      <th>4472</th>\n",
       "      <th>4473</th>\n",
       "      <th>4474</th>\n",
       "      <th>4476</th>\n",
       "      <th>4478</th>\n",
       "      <th>4479</th>\n",
       "      <th>4482</th>\n",
       "      <th>4483</th>\n",
       "      <th>4484</th>\n",
       "      <th>4485</th>\n",
       "      <th>4488</th>\n",
       "      <th>4489</th>\n",
       "      <th>4490</th>\n",
       "      <th>4491</th>\n",
       "      <th>4492</th>\n",
       "      <th>4493</th>\n",
       "      <th>4495</th>\n",
       "      <th>4496</th>\n",
       "      <th>4497</th>\n",
       "      <th>4499</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000079</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000192</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000301</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000387</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000410</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  1     3     5     6     8     ...   4493  4495  4496  4497  4499\n",
       "userId                                 ...                               \n",
       "1000079   NaN   NaN   NaN   NaN   NaN  ...    NaN   NaN   NaN   NaN   NaN\n",
       "1000192   NaN   NaN   NaN   NaN   NaN  ...    NaN   NaN   NaN   NaN   NaN\n",
       "1000301   NaN   NaN   NaN   NaN   NaN  ...    NaN   NaN   NaN   NaN   NaN\n",
       "1000387   NaN   NaN   NaN   NaN   NaN  ...    NaN   NaN   NaN   NaN   NaN\n",
       "1000410   NaN   NaN   NaN   NaN   NaN  ...    NaN   NaN   3.0   NaN   NaN\n",
       "\n",
       "[5 rows x 2861 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data1 = get_user_item_rating_mat(train_data1)\n",
    "# train_data2 = get_user_item_rating_mat(train_data2)\n",
    "matrix_train_data3 = get_user_item_rating_mat(train_data3)\n",
    "# train_data1.sample(4), train_data2.sample(4), train_data3.sample(4)\n",
    "matrix_train_data3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data1.to_csv('train_data1.csv', index=False, header=None)\n",
    "# train_data2.to_csv('train_data2.csv', index=False, header=None)\n",
    "# train_data3.to_csv('train_data3.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_data1, train_data2, train_data3\n",
    "# del train_data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上可知，其中user-item-rating-matrix中有大量的NaN值，对于PureSVD的输入是不合法的，因此若使用PureSVD算法的话，需要对矩阵中的NaN值进行填充。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce83987af321fe8482d8ce49096caa8027dd2dcd"
   },
   "source": [
    "## <a id=7>7. 推荐引擎</a>\n",
    "### <a id=7.1>7.1. Mean Rating</a>\n",
    "使用Mean Rating作为最终的预测结果，这样的结果会导致rating具有偏向性，收视率较高的（即每列中NaN的值较少）会受到影响，使得其rating偏低，进一步让rating结果偏向于收视率较低的rating。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_rating(train, test):\n",
    "    # 0：表示沿着每一列或行标签/索引值向下执行方法\n",
    "    # 1：表示沿着每一行或列标签/索引值向右执行方法\n",
    "    ratings_mean = train.mean(axis=0).rename('rating_mean')\n",
    "    df_pred = test.set_index('movieId').join(ratings_mean)[['rating', 'rating_mean']]\n",
    "#     df_pred.fillna(df_pred.mean(), inplace=True)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true=df_pred['rating'], y_pred=df_pred['rating_mean']))\n",
    "    print(\"mean rating's rmse is {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data3 = pd.read_csv('./train_data3.csv',header=None)\n",
    "# train_data3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data3.index = userId3\n",
    "# train_data3.columns = movieId3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data3 = pd.read_csv('./train_data3.csv', header=None, index_col=userId3.values, names=movieId3.values)\n",
    "# mean_rating_data1 = mean_rating(train_data3, test_data3)\n",
    "# del train_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean rating's rmse is 1.0125986596206626\n"
     ]
    }
   ],
   "source": [
    "# train_data1 = pd.read_csv('./train_data1.csv')\n",
    "# mean_rating_data1 = mean_rating(train_data1, test_data1)\n",
    "# del train_data1\n",
    "# train_data2 = pd.read_csv('./train_data2.csv')\n",
    "# mean_rating_data2 = mean_rating(train_data2, test_data2)\n",
    "# del train_data2\n",
    "# train_data3 = pd.read_csv('./train_data3.csv')\n",
    "# mean_rating_data3 = mean_rating(train_data3, test_data3)\n",
    "# del train_data3\n",
    "mean_rating_data3 = mean_rating(matrix_train_data3, test_data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ebd575a6899487e35c72c96b11b63e3370ad11a9"
   },
   "source": [
    "### <a id=7.2>7.2. [Weighted Mean Rating](https://www.quora.com/How-does-IMDbs-rating-system-work)</a>\n",
    "借助贝叶斯估计（the Bayesian estimate），权重评分公式如下：\n",
    "$$(WR) = \\frac{v}{v+m} \\times R + \\frac{m}{v+m} \\times C$$\n",
    "其中，$R$为电影的平均值，$v$为电影的投票数量，$m$为Top250的最低票数（当前值为25000），$C$为整个数据集的平均票数（当前为7.0）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean_rating(train, test, m=1000):\n",
    "    C = train.stack().mean()  # 一个浮点数\n",
    "    \"\"\"\n",
    "    数据格式如下：\n",
    "    userId1:\n",
    "    movieId11, rating\n",
    "    movieId12, rating\n",
    "    userId2:\n",
    "    movieId21, rating\n",
    "    movieId22, rating\n",
    "    \"\"\"\n",
    "    R = train.mean(axis=0).values # movie个数的一个array，每个值为rating的平均值\n",
    "    v = train.count().values # movie个数的一个array，每个值为user的个数\n",
    "    weighted_score = (v/ (v+m) *R) + (m/ (v+m) *C)\n",
    "    df_prediction = test.set_index('movieId').join(pd.DataFrame(weighted_score, index=train.columns, columns=['prediction']))[['rating', 'prediction']]\n",
    "    y_true = df_prediction['rating']\n",
    "    y_pred = df_prediction['prediction']\n",
    "    rmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\n",
    "    print('weighted mean rating\"s rmse is {}'.format(rmse))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted mean rating\"s rmse is 1.0133315324948378\n"
     ]
    }
   ],
   "source": [
    "weighted_mean_rating_data3 = weighted_mean_rating(matrix_train_data3, test_data3, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$m$是一个超参数，通过调节$m$来改变整体全部评分和每个电影评分的比重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ab47c8afa6b5273fbb7e5eeb7a7020d024d8f72"
   },
   "source": [
    "### <a id=7.3>7.3. Cosine User-User Similarity</a>\n",
    "利用余弦相似度计算用户向量之间的相似度，然后利用这个相似度作为一个电影评分权重和当前电影的评分做加权相乘。\n",
    "$$score=\\frac{\\sum cosine_{ij} rating_{ij}}{\\sum cosine_{ij}}$$\n",
    "需要注意的1）缩放因子；2）和之前两种算法相比更加细化，细化至userId；3）超参数相似度排名top-n。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_u2u_similarity(train, test, n_recommendation=100):\n",
    "    train_imputed = train.T.fillna(train.mean(axis=1)).T  # 利用均值进行填充NaN\n",
    "    similarity = cosine_similarity(train_imputed.values)  # 计算用户之间的余弦相似度\n",
    "    similarity -= np.eye(similarity.shape[0]) # 减去自身相似度\n",
    "    \n",
    "    prediction = []\n",
    "    userId_idx_mapping = {userId:idx for idx, userId in enumerate(train_imputed.index)}\n",
    "    for userId in test.userId.unique():\n",
    "        similarity_user_index = np.argsort(similarity[userId_idx_mapping[userId]])[::-1]\n",
    "        similarity_user_score = np.sort(similarity[userId_idx_mapping[userId]])[::-1]\n",
    "        for movieId in test[test.userId == userId].movieId.values:\n",
    "            \n",
    "            score = (train_imputed.iloc[similarity_user_index[:n_recommendation]][movieId] * similarity_user_score[:n_recommendation]).values.sum() / similarity_user_score[:n_recommendation].sum()\n",
    "            prediction.append([userId, movieId, score])\n",
    "    \n",
    "    # Create prediction DataFrame\n",
    "    df_pred = pd.DataFrame(prediction, columns=['userId', 'movieId', 'prediction']).set_index(['userId', 'movieId'])\n",
    "    df_pred = test.set_index(['userId', 'movieId']).join(df_pred)\n",
    "\n",
    "\n",
    "    # Get labels and predictions\n",
    "    y_true = df_pred['rating'].values\n",
    "    y_pred = df_pred['prediction'].values\n",
    "\n",
    "    # Compute RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\n",
    "    print(\"consine_u2u_similarity's rmse is {}\".format(rmse))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consine_u2u_similarity's rmse is 1.4789047365862287\n"
     ]
    }
   ],
   "source": [
    "cosine_u2u_similarity_data3 = cosine_u2u_similarity(matrix_train_data3, test_data3, n_recommendation=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fea679ee86b2eda4e76f26343dfb2b32ec86ff7f"
   },
   "source": [
    "### <a id=7.4>7.4. Matrix Factorization With Keras And Gradient Descent</a>\n",
    "鉴于user-item-rating是高维且稀疏的矩阵，因此可以用embedding形式表示movieId和userId，然后使用Dot操作去拟合这个user-item-rating矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_dot(train, test, embedding_size=50):\n",
    "    userId_idx_mapping = {userId:idx for idx, userId in enumerate(train.userId.unique())}\n",
    "    movieId_idx_mapping = {movieId:idx for idx, movieId in enumerate(train.movieId.unique())}\n",
    "    # 和reset_index函数一样，为了方便NN模型的输入（主要体现在Batch的获取上）\n",
    "    train_user_data = train.userId.map(userId_idx_mapping)\n",
    "    train_movie_data = train.movieId.map(movieId_idx_mapping)\n",
    "    \n",
    "    test_user_data = test.userId.map(userId_idx_mapping)\n",
    "    test_movie_data = test.movieId.map(movieId_idx_mapping)\n",
    "    \n",
    "    nb_users = len(userId_idx_mapping)\n",
    "    nb_movies = len(movieId_idx_mapping)\n",
    "    \n",
    "    \n",
    "    # 创建模型\n",
    "    # 定义输入，维度\n",
    "    userId_input = Input(shape=[1], name='user')\n",
    "    movieId_input = Input(shape=[1], name='movie')\n",
    "    # 创建embedding层\n",
    "    user_embedding = Embedding(\n",
    "        output_dim=embedding_size,\n",
    "        input_dim=nb_users,\n",
    "        input_length=1,\n",
    "        name='user_embedding'\n",
    "    )(userId_input)\n",
    "    \n",
    "    movie_embedding = Embedding(\n",
    "        output_dim=embedding_size,\n",
    "        input_dim=nb_movies,\n",
    "        input_length=1,\n",
    "        name='movie_embedding'\n",
    "    )(movieId_input)\n",
    "    # Reshape the embedding layers\n",
    "    user_vector = Reshape([embedding_size])(user_embedding)\n",
    "    movie_vector = Reshape([embedding_size])(movie_embedding)\n",
    "\n",
    "    # Compute dot-product of reshaped embedding layers as prediction\n",
    "    y = Dot(1, normalize=False)([user_vector, movie_vector])\n",
    "\n",
    "    # Setup model\n",
    "    model = Model(inputs=[userId_input, movieId_input], outputs=y)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "    # Fit model\n",
    "    model.fit([train_user_data, train_movie_data],\n",
    "              train.rating,\n",
    "              batch_size=256, \n",
    "              epochs=10,\n",
    "              validation_split=0.4,\n",
    "              shuffle=True)\n",
    "\n",
    "    # Test model\n",
    "    y_pred = model.predict([test_user_data, test_movie_data])\n",
    "    y_true = test.rating.values\n",
    "\n",
    "    #  Compute RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n",
    "    print('\\n\\nTesting Result With Keras Matrix-Factorization: {:.4f} RMSE'.format(rmse))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1850542 samples, validate on 1233696 samples\n",
      "Epoch 1/10\n",
      "1850542/1850542 [==============================] - 122s 66us/step - loss: 3.2195 - val_loss: 0.8731\n",
      "Epoch 2/10\n",
      "1850542/1850542 [==============================] - 128s 69us/step - loss: 0.8299 - val_loss: 0.8236\n",
      "Epoch 3/10\n",
      " 962048/1850542 [==============>...............] - ETA: 58s - loss: 0.7760"
     ]
    }
   ],
   "source": [
    "matrix_factorization_dot_train3 = matrix_factorization_dot(train_data3, test_data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "96357ff4ac54e1032d1ba753679efe5aec4b6c51"
   },
   "source": [
    "### <a id=7.6>7.6. Deep Learning With Keras</a>\n",
    "添加模型深度，使用DNN拟合user-item-rating矩阵值，这里仅添加了一个全连接层（dense），使用矩阵拼接作为model的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_dnn(train, test, nb_user_embedding=20, nb_movie_embedding=40):\n",
    "    userId_idx_mapping = {userId:idx for idx, userId in enumerate(train.userId.unique())}\n",
    "    movieId_idx_mapping = {movieId:idx for idx, movieId in enumerate(train.movieId.unique())}\n",
    "    \n",
    "    # Create correctly mapped train- & testset\n",
    "    train_user_data = train.userId.map(userId_idx_mapping)\n",
    "    train_movie_data = train.movieId.map(movieId_idx_mapping)\n",
    "\n",
    "    test_user_data = test.userId.map(userId_idx_mapping)\n",
    "    test_movie_data = test.movieId.map(movieId_idx_mapping)\n",
    "    \n",
    "    nb_users = len(userId_idx_mapping)\n",
    "    nb_movies = len(movieId_idx_mapping)\n",
    "    ##### Create model\n",
    "    # Set input layers\n",
    "    userId_input = Input(shape=[1], name='user')\n",
    "    movieId_input = Input(shape=[1], name='movie')\n",
    "\n",
    "  \n",
    "    \n",
    "    # Create embedding layers for users and movies\n",
    "    user_embedding = Embedding(output_dim=nb_user_embedding, \n",
    "                               input_dim=nb_users,\n",
    "                               input_length=1, \n",
    "                               name='user_embedding')(userId_input)\n",
    "    movie_embedding = Embedding(output_dim=nb_movie_embedding, \n",
    "                                input_dim=nb_movies,\n",
    "                                input_length=1, \n",
    "                                name='item_embedding')(movieId_input)\n",
    "\n",
    "    # Reshape the embedding layers\n",
    "    user_vector = Reshape([nb_user_embedding])(user_embedding)\n",
    "    movie_vector = Reshape([nb_movie_embedding])(movie_embedding)\n",
    "\n",
    "    # Concatenate the reshaped embedding layers\n",
    "    concat = Concatenate()([user_vector, movie_vector])\n",
    "\n",
    "    # Combine with dense layers\n",
    "    dense = Dense(256)(concat)\n",
    "    y = Dense(1)(dense)\n",
    "\n",
    "    # Setup model\n",
    "    model = Model(inputs=[userId_input, movieId_input], outputs=y)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "    # Fit model\n",
    "    model.fit([train_user_data, train_movie_data],\n",
    "              train.rating,\n",
    "              batch_size=256, \n",
    "              epochs=5,\n",
    "              validation_split=0.5,\n",
    "              shuffle=True)\n",
    "\n",
    "    # Test model\n",
    "    y_pred = model.predict([test_user_data, test_movie_data])\n",
    "    y_true = test.rating.values\n",
    "\n",
    "    #  Compute RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n",
    "    print('\\n\\nTesting Result With Keras Deep Learning: {:.4f} RMSE'.format(rmse))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1542119 samples, validate on 1542119 samples\n",
      "Epoch 1/5\n",
      "1542119/1542119 [==============================] - 57s 37us/step - loss: 0.9413 - val_loss: 0.8499\n",
      "Epoch 2/5\n",
      "1542119/1542119 [==============================] - 62s 40us/step - loss: 0.8423 - val_loss: 0.8570\n",
      "Epoch 3/5\n",
      "1542119/1542119 [==============================] - 63s 41us/step - loss: 0.8351 - val_loss: 0.8410\n",
      "Epoch 4/5\n",
      "1542119/1542119 [==============================] - 60s 39us/step - loss: 0.8304 - val_loss: 0.8333\n",
      "Epoch 5/5\n",
      "1542119/1542119 [==============================] - 58s 38us/step - loss: 0.8274 - val_loss: 0.8337\n",
      "\n",
      "\n",
      "Testing Result With Keras Deep Learning: 0.9134 RMSE\n"
     ]
    }
   ],
   "source": [
    "matrix_factorization_dnn_train3 = matrix_factorization_dnn(train_data3, test_data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d9a03d54a7e6f5d315a27a2d53e33ec30d2ddaa"
   },
   "source": [
    "## <a id=8>8. Exploring Python Libraries</a>\n",
    "### <a id=8.1>8.1. Surprise Library</a>\n",
    "[surprise library](http://surpriselib.com/) 是为推荐系统而构建的一个库，有很多内置算法。\n",
    "\n",
    "[SVD](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD)，其预测rating为$\\bar{r}_{ui}=\\mu+b_u+b_i+q_i^Tp_u$，若$u$是未知的，则偏置$b_u$和因子$p_u$假设为0，同理对于item $i$的$b_i$和$q_i$。此时的评估函数如下:\n",
    "$$\\sum_{r_{ui}\\in R_{train}}(r_{ui}-\\bar{ui})^2 + \\lambda(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2)$$\n",
    "使用随机梯度进行参数学习，\n",
    "$$b_u \\gets b_u + \\alpha(e_{ui} - \\lambda b_u)$$\n",
    "$$b_i \\gets b_i + \\alpha(e_{ui} - \\lambda b_i)$$\n",
    "$$p_u \\gets p_u + \\alpha(e_{ui} \\cdot q_i - \\lambda p_u)$$\n",
    "$$q_i \\gets q_i + \\lambda (e_{ui} \\cdot p_u - \\lambda q_i)$$\n",
    "其中$e_{ui}=r_{ui}-\\bar{r}_{ui}$。\n",
    "\n",
    "[SVD++](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp)其中预测rating为$\\bar{r}_{ui}=\\mu + b_u + b_i + q_i^T(p_u + |I_u|^{-\\frac{1}{2}}\\sum_{j \\in I_u}y_j)$，其中$y_j$是一组隐式因子，主要描述了user $u$对item $j$的评价的事实，和评价的rating无关。\n",
    "\n",
    "[Slope One](https://surprise.readthedocs.io/en/stable/slope_one.html)其中预测的rating为$\\bar{r_{ui}}=\\mu_u + \\frac{1}{|R_i(u)|} \\sum_{j \\in R_i(u)} dev(i,j)$，其中$R_i(u)$是item的集合，它是按照user $u$的，并且这个集合user $j$同样评价过，$dev(i,j)$被定义为$dev(i,j) = \\frac{1}{U_{ij}} \\sum_{u \\in U_{ij}} r_{ui}-r_{uj}$。\n",
    "\n",
    "[NMF](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF)其中预测rating为$\\bar{r}_{ui}=q_i^Tp_u$，同样使用随机梯度下降算法，其中item和user的隐因子$f$更新如下：\n",
    "$$p_{uf} \\gets p_{uf} \\cdot \\frac{\\sum_{i \\in I_u}q_{if} \\cdot r_{ui}}{\\sum_{i \\in I_u} q_{if} \\cdot \\bar{r}_{ui} + \\lambda_u |I_u| p_{uf}}$$\n",
    "$$q_{if} \\gets q_{if} \\cdot \\frac{\\sum_{u \\in U_i}p_{uf} \\cdot r_{ui}}{\\sum_{u \\in U_i}p_{uf} \\cdot \\bar{r}_{ui} + \\lambda_i |U_i| q_{if}}$$\n",
    "其中$\\lambda_u$和$\\lambda_i$是超参数，且此算法高度依赖初始化值。\n",
    "\n",
    "[NormalPredictor](https://surprise.readthedocs.io/en/stable/basic_algorithms.html#surprise.prediction_algorithms.random_pred.NormalPredictor)其中预测rating基于正则化的假设上即\n",
    "$$\\begin{split}\\hat{\\mu} &= \\frac{1}{|R_{train}|} \\sum_{r_{ui} \\in R_{train}}\n",
    "r_{ui}\\\\\\\\        \\hat{\\sigma} &= \\sqrt{\\sum_{r_{ui} \\in R_{train}}\n",
    "\\frac{(r_{ui} - \\hat{\\mu})^2}{|R_{train}|}}\\end{split}$$\n",
    " \n",
    "[KNNBasic](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBasic) 其中rating的预测基于KNN的相似性。\n",
    "$$\\hat{r}_{ui} = \\frac{\n",
    "\\sum\\limits_{v \\in N^k_i(u)} \\text{sim}(u, v) \\cdot r_{vi}}\n",
    "{\\sum\\limits_{v \\in N^k_i(u)} \\text{sim}(u, v)}$$\n",
    "\n",
    "[KNNWithMeans](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithMeans)在KNNBasic的基础上添加了均值。\n",
    "$$\\hat{r}_{ui} = \\mu_u + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - \\mu_v)} {\\sum\\limits_{v \\in\n",
    "N^k_i(u)} \\text{sim}(u, v)}$$\n",
    "\n",
    "[KNNWithZScore](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithZScore)和KNNWithMeans相比，对每个user的rating做了$z$-score处理。\n",
    "$$\n",
    "\\hat{r}_{ui} = \\mu_u + \\sigma_u \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - \\mu_v) / \\sigma_v} {\\sum\\limits_{v\n",
    "\\in N^k_i(u)} \\text{sim}(u, v)}\n",
    "$$\n",
    "\n",
    "[KNNBaseline](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline)\n",
    "$$\n",
    "\\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - b_{vi})} {\\sum\\limits_{v \\in\n",
    "N^k_i(u)} \\text{sim}(u, v)}\n",
    "$$\n",
    "\n",
    "[BaselineOnly](https://surprise.readthedocs.io/en/stable/basic_algorithms.html)评估rating为$\\bar{r}_{ui}=b_{ui} =\\mu + b_u + b_i$，当$u$未知的时候，$b_u$假设为0。\n",
    "\n",
    "[CoClustering](https://surprise.readthedocs.io/en/stable/co_clustering.html#surprise.prediction_algorithms.co_clustering.CoClustering)\n",
    "$$\n",
    "\\hat{r}_{ui} = \\overline{C_{ui}} + (\\mu_u - \\overline{C_u}) + (\\mu_i - \\overline{C_i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprise_library(data):\n",
    "    # Load dataset into surprise specific data-structure\n",
    "    sampled_data = sp.Dataset.load_from_df(data[['userId', 'movieId', 'rating']].sample(20000), sp.Reader())\n",
    "\n",
    "    benchmark = []\n",
    "    # Iterate over all algorithms\n",
    "    for algorithm in [sp.SVD(), sp.SVDpp(), sp.SlopeOne(), sp.NMF(), sp.NormalPredictor(), sp.KNNBaseline(), sp.KNNBasic(), sp.KNNWithMeans(), sp.KNNWithZScore(), sp.BaselineOnly(), sp.CoClustering()]:\n",
    "        # Perform cross validation\n",
    "        results = cross_validate(algorithm, sampled_data, measures=['RMSE'], cv=3, verbose=False)\n",
    "\n",
    "        # Get results & append algorithm name\n",
    "        tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "        tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "\n",
    "        # Store data\n",
    "        benchmark.append(tmp)\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n"
     ]
    }
   ],
   "source": [
    "surprise_train3 = surprise_library(filtered_netflix_prize_User)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[test_rmse      1.03665\n",
       " fit_time       1.64775\n",
       " test_time    0.0843676\n",
       " Algorithm          SVD\n",
       " dtype: object, test_rmse    1.03153\n",
       " fit_time      3.0279\n",
       " test_time    0.13135\n",
       " Algorithm      SVDpp\n",
       " dtype: object, test_rmse       1.1954\n",
       " fit_time      0.294815\n",
       " test_time    0.0919317\n",
       " Algorithm     SlopeOne\n",
       " dtype: object, test_rmse      1.20121\n",
       " fit_time       3.48961\n",
       " test_time    0.0812257\n",
       " Algorithm          NMF\n",
       " dtype: object, test_rmse            1.47137\n",
       " fit_time           0.0353295\n",
       " test_time          0.0874324\n",
       " Algorithm    NormalPredictor\n",
       " dtype: object, test_rmse        1.04083\n",
       " fit_time         4.82785\n",
       " test_time       0.197138\n",
       " Algorithm    KNNBaseline\n",
       " dtype: object, test_rmse     1.08784\n",
       " fit_time      4.69928\n",
       " test_time     0.19174\n",
       " Algorithm    KNNBasic\n",
       " dtype: object, test_rmse         1.19282\n",
       " fit_time          4.77909\n",
       " test_time         0.20083\n",
       " Algorithm    KNNWithMeans\n",
       " dtype: object, test_rmse           1.1967\n",
       " fit_time           5.20028\n",
       " test_time         0.194106\n",
       " Algorithm    KNNWithZScore\n",
       " dtype: object, test_rmse         1.03727\n",
       " fit_time        0.0958707\n",
       " test_time       0.0754357\n",
       " Algorithm    BaselineOnly\n",
       " dtype: object, test_rmse         1.17725\n",
       " fit_time          2.10138\n",
       " test_time        0.072137\n",
       " Algorithm    CoClustering\n",
       " dtype: object]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise_train3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b7479d289f96bf71869cf0abc4b07a6773cdf2b"
   },
   "source": [
    "### <a id=8.2>8.2. Lightfm Library</a>\n",
    "[lightfm librariy](https://github.com/lyst/lightfm)重点关注具有显式和隐式的矩阵分解，此外可以利用item等元信息来达到基于内容推荐和协同推荐共同作用的混合模型，从而在一定程度上减少了冷启动的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightfm_library(train, test):\n",
    "    # Create user- & movie-id mapping\n",
    "    user_id_mapping = {id:i for i, id in enumerate(train['userId'].unique())}\n",
    "    movie_id_mapping = {id:i for i, id in enumerate(train['movieId'].unique())}\n",
    "    \n",
    "    # Create correctly mapped train- & testset\n",
    "    train_user_data = train['userId'].map(user_id_mapping)\n",
    "    train_movie_data = train['movieId'].map(movie_id_mapping)\n",
    "\n",
    "    test_user_data = test['userId'].map(user_id_mapping)\n",
    "    test_movie_data = test['movieId'].map(movie_id_mapping)\n",
    "\n",
    "\n",
    "    # Create sparse matrix from ratings\n",
    "    shape = (len(user_id_mapping), len(movie_id_mapping))\n",
    "    train_matrix = coo_matrix((train['rating'].values, (train_user_data.astype(int), train_movie_data.astype(int))), shape=shape)\n",
    "    test_matrix = coo_matrix((test['rating'].values, (test_user_data.astype(int), test_movie_data.astype(int))), shape=shape)\n",
    "\n",
    "\n",
    "    # Instantiate and train the model\n",
    "    model = LightFM(loss='warp', no_components=20)\n",
    "    model.fit(train_matrix, epochs=10, num_threads=2)\n",
    "\n",
    "\n",
    "    # Evaluate the trained model\n",
    "    k = 20\n",
    "    precision_score = precision_at_k(model, test_matrix, k=k).mean()\n",
    "#     print('Train precision at k={}:\\t{:.4f}'.format(k, precision_at_k(model, train_matrix, k=k).mean()))\n",
    "    print('Test precision at k={}:\\t\\t{:.4f}'.format(k, precision_score))\n",
    "    return precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision at k=20:\t\t0.4259\n"
     ]
    }
   ],
   "source": [
    "lightfm_train3 = lightfm_library(train_data3, test_data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=8.3>8.3. DeepCTR</a>\n",
    "[DeepCTR](https://github.com/shenweichen/DeepCTR)是一个基于深度的CTR预测库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DeepFM\n",
    "def deepfm_algo(data):\n",
    "\n",
    "    sparse_features = [\"movieId\", \"userId\"]\n",
    "    target = ['rating']\n",
    "    for feat in sparse_features:\n",
    "            lbe = LabelEncoder()\n",
    "            data[feat] = lbe.fit_transform(data[feat])\n",
    "    \n",
    "    fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique(), embedding_dim=4)\n",
    "                              for feat in sparse_features]\n",
    "    \n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "    \n",
    "    train, test = train_test_split(data, test_size=0.5)\n",
    "    train_model_input = {name:train[name].values for name in feature_names}\n",
    "    test_model_input = {name:test[name].values for name in feature_names}\n",
    "\n",
    "    # 4.Define Model,train,predict and evaluate\n",
    "    model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')\n",
    "    model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "    \n",
    "    history = model.fit(train_model_input, train[target].values,\n",
    "                        batch_size=256, epochs=5, verbose=2, validation_split=0.5, )\n",
    "    pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "    rmse = np.sqrt(mean_squared_error(test[target].values, pred_ans))\n",
    "    print(\"test RMSE\", rmse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1542119 samples, validate on 1542119 samples\n",
      "Epoch 1/5\n",
      " - 39s - loss: 0.9381 - mean_squared_error: 0.9357 - val_loss: 0.8314 - val_mean_squared_error: 0.8267\n",
      "Epoch 2/5\n",
      " - 37s - loss: 0.8004 - mean_squared_error: 0.7932 - val_loss: 0.7888 - val_mean_squared_error: 0.7794\n",
      "Epoch 3/5\n",
      " - 37s - loss: 0.7684 - mean_squared_error: 0.7570 - val_loss: 0.7824 - val_mean_squared_error: 0.7692\n",
      "Epoch 4/5\n",
      " - 40s - loss: 0.7572 - mean_squared_error: 0.7423 - val_loss: 0.7819 - val_mean_squared_error: 0.7656\n",
      "Epoch 5/5\n",
      " - 42s - loss: 0.7503 - mean_squared_error: 0.7326 - val_loss: 0.7826 - val_mean_squared_error: 0.7638\n",
      "test RMSE 0.8748718957924188\n"
     ]
    }
   ],
   "source": [
    "deepfm_algor_train3 = deepfm_algo(filtered_netflix_prize_User)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-c41ffe49f76d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mret_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmean_rating_data3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted_mean_rating_data3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_u2u_similarity_data3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix_factorization_dot_train3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix_factorization_dnn_train3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightfm_train3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepfm_algor_train3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msurprise_train3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RMSE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mret_rmse_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mean_rating'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weighted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cosine_u2u_similarity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mf_dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mf_dnn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lightfm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deepfm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msurprise_train3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RMSE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret_rmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret_rmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtick_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mret_rmse_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "ret_rmse = [mean_rating_data3, weighted_mean_rating_data3, cosine_u2u_similarity_data3, matrix_factorization_dot_train3, matrix_factorization_dnn_train3, lightfm_train3, deepfm_algor_train3] + surprise_train3['RMSE'].tolist() \n",
    "ret_rmse_name = ['mean_rating', 'weighted', 'cosine_u2u_similarity', 'mf_dot', 'mf_dnn', 'lightfm', 'deepfm'] + surprise_train3['RMSE'].tolist()\n",
    "figure, ax = plt.subplots(figsize=(16,4))\n",
    "print(ret_rmse)\n",
    "plt.bar(range(len(ret_rmse)), ret_rmse, tick_label=ret_rmse_name)\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(90)\n",
    "plt.title('Different RMSE in Dataset by RS algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef74632b861afd028442f4a3f44e84297eff03f3"
   },
   "source": [
    "## <a id=9>9. 总结</a>\n",
    "Other **python recommender libraries** are:\n",
    "+ [implicit](https://github.com/benfred/implicit)\n",
    "+ [spotlight](https://github.com/maciejkula/spotlight)\n",
    "+ [turicreate](https://github.com/apple/turicreate/blob/master/README.md)\n",
    "+ [mrec](https://github.com/Mendeley/mrec)\n",
    "+ [recsys](https://github.com/ocelma/python-recsys)\n",
    "+ [crab](http://muricoca.github.io/crab/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
