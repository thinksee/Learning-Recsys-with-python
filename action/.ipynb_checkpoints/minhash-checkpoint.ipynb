{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = ['这个', '程序', '代码', '太乱', '那个', '代码', '规范']\n",
    "data2 = ['这个', '程序', '代码', '不', '规范', '那个', '更', '规范']\n",
    "data3 = ['这个', '程序', '代码', '不', '规范', '那个', '规范', '些']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH, MinHashLSHEnsemble, MinHashLSHForest\n",
    "from simhash import Simhash, SimhashIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建MinHash对象\n",
    "m1 = MinHash()\n",
    "m2 = MinHash()\n",
    "m3 = MinHash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data1:\n",
    "    m1.update(d.encode('utf8'))\n",
    "for d in data2:\n",
    "    m2.update(d.encode('utf8'))\n",
    "for d in data3:\n",
    "    m3.update(d.encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function datasketch.hashfunc.sha1_hash32(data)>,\n",
       " <bound method MinHash.count of <datasketch.minhash.MinHash object at 0x00000246D4EBA888>>,\n",
       " <bound method MinHash.jaccard of <datasketch.minhash.MinHash object at 0x00000246D4EBA888>>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.hashfunc, m1.count, m1.jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinHash预估的Jaccard相似度 0.6015625\n"
     ]
    }
   ],
   "source": [
    "print(\"MinHash预估的Jaccard相似度\", m1.jaccard(m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard相似度实际值 0.625\n",
      "{'程序', '规范', '这个', '代码', '那个'}\n",
      "5\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "s1 = set(data1)\n",
    "s2 = set(data2)\n",
    "actual_jaccard = float(len(s1.intersection(s2)) / float(len(s1.union(s2))))\n",
    "print('Jaccard相似度实际值', actual_jaccard)\n",
    "print(s1.intersection(s2))\n",
    "print(len(s1.intersection(s2)))\n",
    "print(len(s1.union(s2)))\n",
    "print(len(s1.union(s2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "近似邻居（Jaccard相似度>0.5） ['m2', 'm3']\n"
     ]
    }
   ],
   "source": [
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "lsh.insert(\"m2\", m2)\n",
    "lsh.insert(\"m3\", m3)\n",
    "result = lsh.query(m1)\n",
    "print('近似邻居（Jaccard相似度>0.5）', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHashLSHEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.hashvalues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建LSH Ensemble\n",
    "lshensemble = MinHashLSHEnsemble(threshold=0.8, num_perm=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index takes an iterable of (key, minhash, size)\n",
    "lshensemble.index([(\"m2\", m2, len(data2)), (\"m3\", m3, len(data3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, array([6.73794700e-03, 2.04680757e-02, 6.21765240e-02, 1.88875603e-01,\n",
       "        5.73753421e-01, 1.74290900e+00, 5.29449005e+00, 1.60832407e+01,\n",
       "        4.88565713e+01, 1.48413159e+02]), 0.8, array([[ 1,  8],\n",
       "        [ 1,  8],\n",
       "        [ 1,  8],\n",
       "        [ 1,  8],\n",
       "        [ 1,  8],\n",
       "        [25,  4],\n",
       "        [42,  3],\n",
       "        [42,  3],\n",
       "        [42,  3],\n",
       "        [42,  3]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lshensemble.h, lshensemble.xqs, lshensemble.threshold, lshensemble.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{8: <datasketch.lsh.MinHashLSH at 0x246d4ed1188>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4ed1808>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4ed1f48>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4ed3a88>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4ede348>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4ecfac8>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d33667c8>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4ec0548>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4ef4048>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4ef8108>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4ef8988>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4efef48>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4f06048>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4f068c8>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4f0ce88>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4f0ffc8>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4f15848>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4f1ae08>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4f1cec8>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4f23788>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4f28d48>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4f2ae08>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4f306c8>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4f36c88>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4f39d48>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4f3f608>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4f43bc8>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4f0ff48>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4f4c548>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4f53b88>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4f56cc8>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4f5a608>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4f60c48>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4f64d88>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4f676c8>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4f6ed08>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4f72e48>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4f76788>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4f7cdc8>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4f7ff08>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4f83848>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4f88e88>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4f8cfc8>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4f90908>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4f96f48>},\n",
       " {8: <datasketch.lsh.MinHashLSH at 0x246d4fa00c8>,\n",
       "  3: <datasketch.lsh.MinHashLSH at 0x246d4fa09c8>,\n",
       "  4: <datasketch.lsh.MinHashLSH at 0x246d4fa9048>}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lshensemble.indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 判断lshensemble是否存在m2, m3\n",
    "print(\"m2\" in lshensemble)\n",
    "print(\"m3\" in lshensemble)\n",
    "print(\"m1\" in lshensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与m1相似度大于0.8的集合:\n",
      "m2\n",
      "m3\n"
     ]
    }
   ],
   "source": [
    "print('与m1相似度大于0.8的集合:')\n",
    "for key in lshensemble.query(m1, len(data1)):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHashLSHForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = MinHashLSHForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.add(\"m2\", m2)\n",
    "forest.add(\"m3\", m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在检索前，需要使用index\n",
    "forest.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('m1' in forest)\n",
    "print('m2' in forest)\n",
    "print('m3' in forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2: ['m2', 'm3']\n"
     ]
    }
   ],
   "source": [
    "result = forest.query(m1, 2)\n",
    "print('Top 2:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHashLSHForest + news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba.posseg as pseg\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/sentences.txt', encoding='UTF-8') as file:\n",
    "    text = file.read()\n",
    "sentences = re.split('[。！？]', text.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11月10日深夜到11日凌晨，杭州阿里巴巴园区光柱擎天、灯火辉煌、不眠不休',\n",
       " '这是第11年双11，一个稀疏平常的日子凭空成为全民购物狂欢本就不可思议，而更不可思议的是不断刷新的消费数据',\n",
       " '仅用时96秒，2019天猫双11总成交额便超过100亿元，这个速度，比2018年缩短了29秒',\n",
       " '随后，500亿元、1000亿元，每一个成交额小目标都以更快的速度实现',\n",
       " '截至中证君发稿，2019天猫双11前三小时总成交额达到1325.7亿元，已经超过2018年天猫双11全天2135亿元总成交额的62%',\n",
       " '2018年，阿里巴巴用15小时49分钟39秒时间，打破了2017年1682亿元的双11全天成交纪录，最终以27%的增速定格在2135亿元',\n",
       " '今年双11，阿里巴巴又将以怎样的增速、多长时间来打破自己的纪录，它能否一举突破3000亿元大关，同时提振全国内需',\n",
       " '作为双11购物狂欢盛典的鼻祖，阿里巴巴在过去三年双11全网成交额中占据三分之二天下，京东、苏宁以及近期势头凶猛的拼多多均处于“后来”但尚未“居上”状态',\n",
       " '首先来看阿里巴巴（天猫）战报',\n",
       " '据介绍，今年，超过1000万款商品登陆天猫双11，超100万款新品在天猫双11首发，来自78个国家和地区的跨境电商进口商品、全国1000个数字农业基地的高品质农产品、2000个传统制造业产业带的工厂直供好货，以新供给满足消费者的多元化需求',\n",
       " '00:01:36，2019天猫双11总成交额超100亿元',\n",
       " '2018年用时2分05秒，今年缩短29秒',\n",
       " '00:12:49，2019天猫双11总成交额超500亿元',\n",
       " '2018年用时26分3秒，今年缩短一半',\n",
       " '01:03:59，2019天猫双11总成交额超1000亿元',\n",
       " '比2018年快了43分钟27秒，比2017年快了将近8小时',\n",
       " '']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11月10日深夜到11日凌晨，杭州阿里巴巴园区光柱擎天、灯火辉煌、不眠不休',\n",
       " '这是第11年双11，一个稀疏平常的日子凭空成为全民购物狂欢本就不可思议，而更不可思议的是不断刷新的消费数据',\n",
       " '仅用时96秒，2019天猫双11总成交额便超过100亿元，这个速度，比2018年缩短了29秒',\n",
       " '随后，500亿元、1000亿元，每一个成交额小目标都以更快的速度实现',\n",
       " '截至中证君发稿，2019天猫双11前三小时总成交额达到1325.7亿元，已经超过2018年天猫双11全天2135亿元总成交额的62%',\n",
       " '2018年，阿里巴巴用15小时49分钟39秒时间，打破了2017年1682亿元的双11全天成交纪录，最终以27%的增速定格在2135亿元',\n",
       " '今年双11，阿里巴巴又将以怎样的增速、多长时间来打破自己的纪录，它能否一举突破3000亿元大关，同时提振全国内需',\n",
       " '作为双11购物狂欢盛典的鼻祖，阿里巴巴在过去三年双11全网成交额中占据三分之二天下，京东、苏宁以及近期势头凶猛的拼多多均处于“后来”但尚未“居上”状态',\n",
       " '首先来看阿里巴巴（天猫）战报',\n",
       " '据介绍，今年，超过1000万款商品登陆天猫双11，超100万款新品在天猫双11首发，来自78个国家和地区的跨境电商进口商品、全国1000个数字农业基地的高品质农产品、2000个传统制造业产业带的工厂直供好货，以新供给满足消费者的多元化需求',\n",
       " '00:01:36，2019天猫双11总成交额超100亿元',\n",
       " '2018年用时2分05秒，今年缩短29秒',\n",
       " '00:12:49，2019天猫双11总成交额超500亿元',\n",
       " '2018年用时26分3秒，今年缩短一半',\n",
       " '01:03:59，2019天猫双11总成交额超1000亿元',\n",
       " '比2018年快了43分钟27秒，比2017年快了将近8小时']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = [line.strip() for line in open('dataset/stopwords/baidu_stopwords.txt', encoding='UTF-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1396"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_text进行切词\n",
    "def get_item_str(item_text):\n",
    "    item_str = \"\"\n",
    "    item = (pseg.cut(item_text))\n",
    "    for i in list(item):\n",
    "        # 去掉停用词\n",
    "        if i.word not in list(stop) and i.word not in ['', ' ', '#', '，','：','《','》', '…']:\n",
    "            item_str += i.word\n",
    "            # tfidf_vectorized.fit_transform的输入需要空格分隔的单词\n",
    "            item_str += ' '\n",
    "    return item_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对item_str创建MinHash\n",
    "def get_minhash(item_str):\n",
    "    tmp = MinHash()\n",
    "    for d in item_str:\n",
    "        tmp.update(d.encode('utf-8'))\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到分词后的documents\n",
    "documents = []\n",
    "for item_text in sentences:\n",
    "    item_str = get_item_str(item_text)\n",
    "    documents.append(item_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11 月 10 日 深夜 11 日 凌晨 杭州 阿里巴巴 园区 光柱 擎天 、 灯火辉煌 、 不眠不休 ',\n",
       " '11 年 双 11 一个 稀疏 平常 日子 凭空 全民 购物 狂欢 不可思议 更 不可思议 刷新 消费 数据 ',\n",
       " '仅 时 96 秒 2019 天猫 双 11 总 成交额 便 超过 100 亿元 速度 2018 年 缩短 29 秒 ',\n",
       " '随后 500 亿元 、 1000 亿元 一个 成交额 小 目标 都 更快 速度 ',\n",
       " '截至 中证君 发稿 2019 天猫 双 11 前 三 小时 总 成交额 1325.7 亿元 超过 2018 年 天猫 双 11 全天 2135 亿元 总 成交额 62 % ',\n",
       " '2018 年 阿里巴巴 15 小时 49 分钟 39 秒 时间 打破 2017 年 1682 亿元 双 11 全天 成交 纪录 最终 27 % 增速 定格 2135 亿元 ',\n",
       " '双 11 阿里巴巴 增速 、 多长时间 打破 纪录 一举 突破 3000 亿元 大关 提振 全国 内需 ',\n",
       " '双 11 购物 狂欢 盛典 鼻祖 阿里巴巴 三年 双 11 全网 成交额 中 占据 三分之二 天下 京东 、 苏宁 近期 势头 凶猛 拼 多多 均 处于 尚未 居 上 状态 ',\n",
       " '来看 阿里巴巴 （ 天猫 ） 战报 ',\n",
       " '据介绍 超过 1000 万款 商品 登陆 天猫 双 11 超 100 万款 新品 天猫 双 11 首发 来自 78 国家 地区 跨境 电商 进口商品 、 全国 1000 数字 农业 基地 高品质 农产品 、 2000 传统 制造业 产业带 工厂 直供 好 货 新 供给 消费者 多元化 需求 ',\n",
       " '00 : 01 : 36 2019 天猫 双 11 总 成交额 超 100 亿元 ',\n",
       " '2018 年 时 2 分 05 秒 缩短 29 秒 ',\n",
       " '00 : 12 : 49 2019 天猫 双 11 总 成交额 超 500 亿元 ',\n",
       " '2018 年 时 26 分 3 秒 缩短 一半 ',\n",
       " '01 : 03 : 59 2019 天猫 双 11 总 成交额 超 1000 亿元 ',\n",
       " '2018 年 快 43 分钟 27 秒 2017 年 快 将近 8 小时 ']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建LSH Forest及MinHash对象\n",
    "minhash_list = []\n",
    "forest = MinHashLSHForest()\n",
    "for i in range(len(documents)):\n",
    "    tmp = get_minhash(documents[i])\n",
    "    minhash_list.append(tmp)\n",
    "    forest.add(i, tmp)\n",
    "# index所有key, 以便可以进行检索\n",
    "forest.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.84421293528233"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minhash_list[0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '2019天猫双11总成交额超过100亿元'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_str = get_item_str(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash_query = get_minhash(item_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.78125 00:01:362019天猫双11总成交额超100亿元\n",
      "12 0.828125 00:12:492019天猫双11总成交额超500亿元\n",
      "14 0.78125 01:03:592019天猫双11总成交额超1000亿元\n",
      "Top 3 [10, 12, 14]\n"
     ]
    }
   ],
   "source": [
    "# 查询forest中与m1相似的top-k个邻居\n",
    "result = forest.query(minhash_query, 3)\n",
    "for i in range(len(result)):\n",
    "    print(result[i], minhash_query.jaccard(minhash_list[result[i]]), documents[result[i]].replace(' ', ''))\n",
    "print('Top 3', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    '这个程序代码太乱,那个代码规范',\n",
    "    '这个程序代码不规范,那个更规范',\n",
    "    '我是佩奇，这是我的弟弟乔治'\n",
    "]\n",
    "\n",
    "data = [\n",
    "    '这个 程序 代码 太乱 那个 代码 规范',\n",
    "    '这个 程序 代码 不 规范 那个 更 规范',\n",
    "    '我 是 佩奇 这 是 我的 弟弟 乔治'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()\n",
    "D = vec.fit_transform(data)\n",
    "voc = dict((i, w) for w, i in vec.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.32060325720533567\n",
      "  (0, 9)\t0.32060325720533567\n",
      "  (0, 3)\t0.4215547553457735\n",
      "  (0, 1)\t0.6412065144106713\n",
      "  (0, 6)\t0.32060325720533567\n",
      "  (0, 8)\t0.32060325720533567\n"
     ]
    }
   ],
   "source": [
    "print(D.getrow(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 9, 3, 1, 6, 8])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.getrow(0).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: '这个',\n",
       " 6: '程序',\n",
       " 1: '代码',\n",
       " 3: '太乱',\n",
       " 9: '那个',\n",
       " 7: '规范',\n",
       " 2: '佩奇',\n",
       " 5: '我的',\n",
       " 4: '弟弟',\n",
       " 0: '乔治'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.32060325720533567\n",
      "  (0, 9)\t0.32060325720533567\n",
      "  (0, 3)\t0.4215547553457735\n",
      "  (0, 1)\t0.6412065144106713\n",
      "  (0, 6)\t0.32060325720533567\n",
      "  (0, 8)\t0.32060325720533567 \n",
      " <zip object at 0x00000246E17ABFC8>\n",
      "  (0, 7)\t0.7071067811865476\n",
      "  (0, 9)\t0.3535533905932738\n",
      "  (0, 1)\t0.3535533905932738\n",
      "  (0, 6)\t0.3535533905932738\n",
      "  (0, 8)\t0.3535533905932738 \n",
      " <zip object at 0x00000246E1783808>\n",
      "  (0, 0)\t0.5\n",
      "  (0, 4)\t0.5\n",
      "  (0, 5)\t0.5\n",
      "  (0, 2)\t0.5 \n",
      " <zip object at 0x00000246E17832C8>\n"
     ]
    }
   ],
   "source": [
    "# 生成Simhash\n",
    "sh_list = []\n",
    "for i in range(D.shape[0]):\n",
    "    Di = D.getrow(i)\n",
    "    # features表示 (token, weight)元祖形式的列表\n",
    "    features = zip([voc[j] for j in Di.indices], Di.data)\n",
    "    print(Di, '\\n', features)\n",
    "    sh_list.append(Simhash(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<simhash.Simhash object at 0x00000246E17ABF08>\n"
     ]
    }
   ],
   "source": [
    "print(sh_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "32\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "print(sh_list[0].distance(sh_list[1]))\n",
    "print(sh_list[0].distance(sh_list[2]))\n",
    "print(sh_list[1].distance(sh_list[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weibo.txt 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/weibos.txt', encoding='UTF-8') as file:\n",
    "    text = file.read()\n",
    "weibo = re.split('[。！？]', text.replace('\\n', '').replace('\\u200b', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到分词后的documents\n",
    "weibo_documents = []\n",
    "for item_text in weibo:\n",
    "    item_str = get_item_str(item_text)\n",
    "    weibo_documents.append(item_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['斯科拉里 执教 国足 上 一届 里 皮 从头 芾 尾 很大 机会 入 世界杯 一届 没 几个 能用 归化 都 没用 ',\n",
       " '国 足 输给 叙利亚 里 皮 辞职 ',\n",
       " '新 主帅 球迷 关注 焦点 ',\n",
       " '舆论 倾向 三个 人 山东鲁能 主帅 李霄鹏 、 武汉 卓尔 主帅 李铁 、 前 广州 恒大 主帅 斯科拉里 ',\n",
       " '中国足协 态度 里 皮 请辞 去 意 已 决 ',\n",
       " '',\n",
       " '比赛 当晚 太太 西蒙内塔 女士 儿子 小里皮 都 现场 看 台上 观战 ',\n",
       " '辞职 后 里皮 改变 原有 计划 — — 赛后 第二天 会 迪拜 飞 回 意大利 ',\n",
       " '意味着 本来 没 打算 球队 管理层 中国足协 高层 赛后 第一 时间 内 辞职 对话 ',\n",
       " '辞职 善后工作 包括 合同 沟通 工作 要待 日后 进一步 协商 ',\n",
       " '回顾 国 足 历届 外籍 教练 — — 里 皮 佩兰 卡马乔 杜伊 科维奇 阿里 · 汉 米卢 ',\n",
       " '一个 一个 有名 一个 一个 水 国足 踢 不好 足协 足协 不 解散 重组 天王老子 请来 都 不行 斯科拉里 想 执教 中国 国 足 ',\n",
       " '老头 凡是 里 皮 干 地方 想 试试 ',\n",
       " '老头 世界杯 冠军 教头 折 中国 没 丢人 毕竟 里 皮 折 ',\n",
       " '试试 ',\n",
       " '斯科拉里 水平 还 里 皮 ',\n",
       " '斯科拉里 看好 国 足 年薪 辞职 ',\n",
       " '中国 足球 不 名帅 不 外籍 教练 一点儿 毛用 ',\n",
       " '施拉普纳 二十余年 间 中国 足球 竟然 大踏步 倒退 一点儿 杀 不住 车 奶奶 刹车 系统 坏 ',\n",
       " '穿着 几百块 钱 球衣 几千块 钱 球鞋 几万块 钱 包 几十万 包机 几百万 上 千万 年薪 赛后 叙利亚 主教练 更衣室 里 队员 一个 耳光 ',\n",
       " '主教练 说 赛前 老子 再三 交代 一场 无论如何 都 赢 中国队 ',\n",
       " '中国 援助 粮食 美金 不再 援助 国家 狗日 些 吃 土 去 ',\n",
       " '球员 委屈 说 七十多 分钟 晓得 龟儿子 家 球门 踢 ',\n",
       " '里 皮 辞职 返回 意大利 助教 马达 洛尼 随队 返回 广州 ',\n",
       " '马达 洛尼 接受 采访 时 还原 更衣室 中 情况 更衣室 球员 都 试图 说服 里 皮 收回 队长 郑智 尝试 阻止 足协 代表 希望 考虑一下 建议 重新考虑 无济于事 ',\n",
       " '中国足协 接受 里 皮 辞职 请求 深刻反思 看 报道 马达 洛尼 说 里 皮 辞职 事先 告知 不 发生 也许 里 皮 头脑 一热 ',\n",
       " '足协 留下 教练组 成员 都 留下 教练 合作 ',\n",
       " '中国队 斯科拉里 开足 薪水 足 条件 ',\n",
       " '一句 话 看好 中国队 潜力 句 话 真是太 鼓舞人心 ',\n",
       " '带队 绝不会 比里皮 更 差 能带 国 足 夺得 2022 世界杯 冠军 ',\n",
       " '支持 斯科拉里 不 ',\n",
       " '世界 名帅 里 皮 带领 国 足 走出 迷茫 国足 蒸蒸日上 国人 不再 迷茫 吃 下 一颗 定心丸 ',\n",
       " '才 最 应 改变 教练 国足 训练 机制 、 学习 机制 、 培养 机制 ',\n",
       " '做好 才能 使 哪位 名帅 接手 都 游刃有余 打好 比赛 ',\n",
       " '国 足 输给 叙利亚 后 里 皮 坐不住 辞职 难怪 网友 说 爱护 生命 远离 男足 ',\n",
       " '男足 水平 南极洲 企鹅 踢球 ',\n",
       " '足协 主席 赠书 红星 去 送给 中国 国脚 埃 尔克 森 、 里 皮 懂 红色 文化 ',\n",
       " '国 足 昨晚 1 - 2 输给 叙利亚 赛后 主帅 里 皮 辞职 ',\n",
       " '']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建LSH Forest及MinHash对象\n",
    "minhash_list = []\n",
    "forest = MinHashLSHForest()\n",
    "for i in range(len(weibo_documents)):\n",
    "    tmp = get_minhash(weibo_documents[i])\n",
    "    minhash_list.append(tmp)\n",
    "    forest.add(i, tmp)\n",
    "# index所有key, 以便可以进行检索\n",
    "forest.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '2022年世界杯冠军是谁？'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash_query = get_minhash(get_item_str(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 0.0546875 斯科拉里看好国足年薪辞职\n",
      "13 0.2265625 老头世界杯冠军教头折中国没丢人毕竟里皮折\n",
      "29 0.328125 带队绝不会比里皮更差能带国足夺得2022世界杯冠军\n",
      "Top 3 [16, 13, 29]\n"
     ]
    }
   ],
   "source": [
    "# 查询forest中与m1相似的top-k个邻居\n",
    "result = forest.query(minhash_query, 3)\n",
    "for i in range(len(result)):\n",
    "    print(result[i], minhash_query.jaccard(minhash_list[result[i]]), weibo_documents[result[i]].replace(' ', ''))\n",
    "print('Top 3', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 13, 29]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
