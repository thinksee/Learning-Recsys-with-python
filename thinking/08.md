### Thinking

**1. 在CTR点击率预估中，使用GBDT+LR的原理是什么？**

其中GBDT的主要职责是构建新的特征，其中每个样本都会进入到一个叶子节点内，并且这个叶子节点都有唯一的编号（在这个情况下，树是学习的残差，因此每个样本针对每棵树都会有一个唯一的叶子标示，并且这个叶子标示可能会出现重复的状况），若设置n_estimator=n，则此时的样本特征数为n。为了统一所有的样本特征使用one-hot进行编码，此时的特征数将会大于等于n（并且此时的特征向量较为稀疏）。

对于稀疏的向量可以使用逻辑回归（LR）进行学习。

其中在”Practical Lessons from Predicting Clicks on Ads at Facebook, 2014“，还提到在工程实现的时候使用Normalized Cross-Entropy(NE) = $\frac{预测的logloss}{backgoround CTR的熵}=$每次展现时预测得到的log loss的平均值，除以对整个数据集的平均log loss值，其中NE数值越小预测效果越好。

**2. Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）**

**memorization**记忆能力，学习items或者features之间的相关频率，在历史数据中探索相关性的可行性；

**generalization**泛化能力，基于相关性的传递，去探索一些在过去没有出现过的特征组合。

- Wide，系统通过获得用户的购物日志数据，包括用户点击哪些商品，购买过哪些商品，然后通过one-hot编码转换为离散特征。好处是可解释性强，不足在于特征组合需要人为操作。可以采用Linear Regression，特征组合需要人为设计，实际中还需要交叉特征，最终的wide模型可以表示为$y=W^TX + b +\sum_{k=1}^N w_{d+k}\phi_k$。在wide模型端，输入特征可以是连续特征，也可以是稀疏的离散特征。离散特征通过交叉可以组合成更高维的离散特征。
- Deep，通过深度学习出一些向量，这些向量是隐形特征，往往没有可解释的。此端使用的特征有连续特征，Embedding后的离散特征，使用前馈网络模型，特征首先转换为低维稠密向量，作为第一个隐藏层的输入，解决维度爆炸的问题。
- ensemble，wide和deep模型分别对全量数据进行预测，然后根据权重组合最终的预测结果。
- wide部分主要是通过cross product的形式生成组合特征，但是无法学习到训练集中没有出现的组合特征。

**3. 在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？**

- FNN模型，鉴于CTR中大部分特征是离散、高维且稀疏的，需要embedding后才能进行深度学习，因此可以使用FM对embedding层进行初始化，即每个特征对应一个偏置项w和一个k维向量v。FNN实际上是wide&deep模型中的FM参数初始化的deep模型；和FM进行串行。
- DeepFM是将wide&Deep模型中的wide替换成了FM模型，因为FM可以学习到LR不能学习到的二阶交叉项。和FM进行并行。
- NFM模型，Wide&Deep，DeepFM都是在DNN部分，对embedding之后的特征进行concatenate，没有充分进行特征交叉计算。NFM模型是对embedding直接采用对位相乘（element-wise）后相加起来作为交叉特征，然后通过DNN直接将特征压缩，最后concatenate linear部分和deep部分的特征，NFM也是串行结构，将FM的结果作为DNN的输入。
- PNN模型，认为在embedding输入到MLP之后的交叉特征表达并不充分，提出一种product layer的思想，即基于乘法的运算来体现交叉的DNN网络结构，其中l1层的输出由以下公式构成$l_1 = relu(l_z + l_p + b_1)$，其中$l_z$表示embedding层的隐向量，表示线性部分，$l_p$分别可以由内积和外积的形式构成，其中内积的结果为一个值，外积的结果为一个矩阵，然后使用叠加的思想，重新定义矩阵。
- AFM模型，也是一种串行的网络结构。在CTR中，为了解决稀疏特征的问题，学者们提出了FM进行特征之间的建模，但是FM模型只能表达特征之间两两组合之间的关系，无法建模两个特征之间深层次的关系，即不同特征之间交互的重要性。其中该模型只要是在FM的基础上添加了attention的机制，但是实际上，由于最后的加权累计，二次项并没有进行更深的网络去学习非线性交叉特征，所以AFM并没有发挥出DNN的优势。
- xDeepFM模型，鉴于当前的基本模型学习架构为首先因子分解机，然后利用全连接神经网络去自动学习特征间的高阶交互关系，如FNN，PNN和DeepFM等，其缺点是模型学习出的隐式的交叉特征，其形式是未知的、不可控的，同时这些模型的特征交互是发生在元素级而不是特征向量之间。xDeepFM使得模型特征交互发生在特征向量级，而且能同时以显式和隐式的方式自动学习高阶的特征交互，使特征交互发生在向量级，还兼具记忆与泛化的学习能力。其中后一层的结果由前一层的结果得来，最后的结果是每个隐藏层求sum pooling，xDeepFM除了DNN，FM还有CIN层。

**4. Surprise工具中的baseline算法原理是怎样的？BaselineOnly和KNNBaseline有什么区别？**

- Baseline算法，基于统计的基准预测线打分，基本公式$b_{ui}=\mu+b_u+b_i$，其中$b_{ui}$表示预测值，$b_u$表示用户对整体的偏差，$b_i$表示商品对整体的偏差。

  该算法使用的ALS进行优化，1. 固定$b_u$，优化$b_i$；2. 固定$b_i$，优化$b_u$

- UserCF，1.利用Jaccard找到和目标用户兴趣相似的用户集合; 2. 用户$u$对物品$i$的相似性，等价于$K$个邻居对物品$i$的兴趣度；3. 为用户$u$生成推荐列表；

- ItemCF，1.计算物品之间的相似度；2. 用户u对物品i的兴趣度，等价于物品i的k个邻居物品，受到用户u的兴趣度；3. 为用户u生成推荐列表。

- KNNBaseline和BaselineOnly的思想类似，但是具体实现不一样。BaselineOnly评分为所有打分的均值，用户对整体的偏差以及商品对整体的偏差三者之和。KNNBaseline借助其他相似用户的评分，会在使用的时候将其他用户对整体的偏差减掉，在计算完相似单元之后，在最后再加上该用户对整体的偏差。

**5. GBDT和随机森林都是基于树的算法，它们有什么区别？**

- GBDT是基于boosting的，而RF是基于bagging的；

  GBDT每次优化的都是上一棵树的残差，而RF是对每棵树同时进行优化，最后的结果GBDT是所有树的累加和，而RF可以采用投票或者加权求和的方式。

- GBDT的基本树单元是回归树，而RF可以是回归树也可以是分类树;

- GBDT一般很难并行实现，RF可以并行训练。

**6. 基于邻域的协同过滤都有哪些算法，请简述原理**

- KNNBasic: 可以用作基于user的协同过滤，也可以用作基于item的协同过滤。其中userCF通过计算用户之间相似度和其他用户对某item的打分的乘积的累加和，再除以用户之间的相似度，可以预测该用户对某item的评分。itemCF同理。
- KNNWithMeans: 在KNNBasic的基础上，计算评分的时候减去了其他用户评分的均值，在最终评分的时候加上该用户的均值，这样提升了用户的个性化。
- KNNBaseline：和BaselineOnly的思想类似。考虑了用户对整体的偏差。计算时，相似用户减去偏差，最后再加该用户的偏差。

